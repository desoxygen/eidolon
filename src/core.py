import ollama
import json
import yaml
from pathlib import Path
from src.memory import MemoryEngine

# –ò–º–ø–æ—Ä—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
try:
    from src.tools import AVAILABLE_TOOLS, get_tools_description
except ImportError:
    AVAILABLE_TOOLS = {}
    def get_tools_description(tools): return ""

class EidolonCore:
    # –ò–ó–ú–ï–ù–ï–ù–ò–ï 1: –¢–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ–º profile_folder="Friend"
    def __init__(self, profile_folder="Friend"):
        print("‚öôÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ø–¥—Ä–∞...")
        self.load_persona(profile_folder)

    def load_persona(self, folder_name):
        print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –ü–µ—Ä—Å–æ–Ω—ã –∏–∑ –ø–∞–ø–∫–∏: {folder_name}...")
        
        self.base_dir = Path(__file__).parent.parent
        
        # 1. –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
        persona_dir = self.base_dir / "profiles" / folder_name
        
        # 2. –ò—â–µ–º –≤–Ω—É—Ç—Ä–∏ config.yaml
        config_path = persona_dir / "core_persona.yaml"
        
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                self.persona = yaml.safe_load(f)
        except FileNotFoundError:
            print(f"‚ùå –û—à–∏–±–∫–∞: –ö–æ–Ω—Ñ–∏–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {config_path}")
            # –ê–≤–∞—Ä–∏–π–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞
            self.persona = {
                "name": "Error",
                "role": "System",
                "system_prompt": "Error loading profile.",
                "allowed_tools": []
            }

        # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ú–æ–¥–µ–ª—å
        self.current_model = self.persona.get("model", "llama3.1")
        
        # 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ü–∞–º—è—Ç—å –í–ù–£–¢–†–ò –ø–∞–ø–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
        # –¢–µ–ø–µ—Ä—å –±–∞–∑–∞ –ª–µ–∂–∏—Ç –≤ Eidolon/data/profiles/Friend/memory_db
        memory_path = persona_dir / "memory_db"
        self.memory = MemoryEngine(db_path=memory_path)

        # 5. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
        self.allowed_tools = self.persona.get("allowed_tools", [])

        print(f"üë§ –õ–∏—á–Ω–æ—Å—Ç—å: {self.persona.get('name')}")
        print(f"üß† –Ø–¥—Ä–æ: {self.current_model}")
        print(f"üìÇ –ü–∞–ø–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {persona_dir}")

    def chat(self, user_input):
        print(f"\nüó£Ô∏è User: {user_input}")

        # 1. RAG
        found_memories = self.memory.search(user_input, limit=2)
        context_str = "\n".join([f"- {m}" for m in found_memories]) if found_memories else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö."

        # 2. –ü—Ä–æ–º–ø—Ç
        tools_instruction = get_tools_description(self.allowed_tools)

        system_msg = f"""
        –¢–´: {self.persona.get('system_prompt', '')}
        –ü–†–û–§–ò–õ–¨: –ò–º—è: {self.persona.get('name')}, –¢–æ–Ω: {self.persona.get('tone', 'Normal')}
        –§–ê–ö–¢–´ –ò–ó –ü–ê–ú–Ø–¢–ò: 
        {context_str}
        
        {tools_instruction}
        """

        messages = [
            {'role': 'system', 'content': system_msg},
            {'role': 'user', 'content': user_input},
        ]

        # 3. –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å (–±–µ–∑ —Å—Ç—Ä–∏–º–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ JSON)
        print(f"ü¶ô –ó–∞–ø—Ä–æ—Å –∫ {self.current_model}...")
        try:
            response = ollama.chat(model=self.current_model, messages=messages, stream=False)
            reply = response['message']['content']
        except Exception as e:
            yield f"–û—à–∏–±–∫–∞ —Å–≤—è–∑–∏ —Å Ollama: {e}"
            return

        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ Tool Use
        if reply.strip().startswith('{') and '"tool":' in reply:
            try:
                print(f"üîß Tool Call: {reply}")
                tool_data = json.loads(reply)
                tool_name = tool_data.get("tool")
                tool_args = tool_data.get("args")

                if tool_name in AVAILABLE_TOOLS and tool_name in self.allowed_tools:
                    tool_func = AVAILABLE_TOOLS[tool_name]
                    tool_result = tool_func(tool_args) if tool_args else tool_func()
                    print(f"‚úÖ Result: {tool_result}")

                    messages.append({'role': 'assistant', 'content': reply})
                    messages.append({'role': 'user', 'content': f"SYSTEM: –†–µ–∑—É–ª—å—Ç–∞—Ç: {tool_result}. –î–∞–π –æ—Ç–≤–µ—Ç."})

                    stream = ollama.chat(model=self.current_model, messages=messages, stream=True)
                    full_reply = ""
                    for chunk in stream:
                        part = chunk['message']['content']
                        full_reply += part
                        yield part
                    
                    self.memory.save(f"Q: {user_input}\nTool: {tool_name}\nA: {full_reply}", type="tool_chat")
                    return

            except Exception as e:
                print(f"‚ùå Tool Error: {e}")
                yield f"[–û—à–∏–±–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {e}]"
                return

        # 5. –û–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç
        yield reply
        self.memory.save(f"User: {user_input}\nEidolon: {reply}", type="chat_history")